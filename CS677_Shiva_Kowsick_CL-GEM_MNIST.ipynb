{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Shiva_DL",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNpxQxlZ5fZC"
      },
      "source": [
        "# CONTINUAL LEARNING\n",
        "**OPTION** : MNIST\n",
        "**MEMBERS** : Shivakalyan Soundarathiagarajan and Kowsick Venkatachalapathi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hShHlhTpjbL2"
      },
      "source": [
        "**Gradient Episodic Memory**\n",
        "\n",
        "1.Continual learning, where the model observes, once and one by one, examples concerning a sequence of tasks.\n",
        "\n",
        "2.Set of metrics to evaluate models learning over a continuum of data. \n",
        "\n",
        "3.Gradient Episodic Memory (GEM) alleviates forgetting, while allowing beneficial transfer of knowledge to previous tasks. Experiments on variants of the MNIST and CIFAR-100 datasets demonstrate the strong performance of GEM when compared to the state-of-the-art.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHK966dKiGPL",
        "outputId": "626ca7bf-6048-4397-c12c-8a0c4ab9f265",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oMQVwWKruu0"
      },
      "source": [
        "MY_PYTHON=\"!python\" #TODO: modified to add\n",
        "MNIST_ROTA=\"--n_layers 2 --n_hiddens 100 --data_path data/ --save_path results/ --batch_size 10 --log_every 100 --samples_per_task 1000 --data_file mnist_rotations.pt    --cuda no  --seed 0\"\n",
        "MNIST_PERM=\"--n_layers 2 --n_hiddens 100 --data_path data/ --save_path results/ --batch_size 10 --log_every 100 --samples_per_task 1000 --data_file mnist_permutations.pt --cuda no  --seed 0\"\n",
        "CIFAR_100i=\"--n_layers 2 --n_hiddens 100 --data_path data/ --save_path results/ --batch_size 10 --log_every 100 --samples_per_task 2500 --data_file cifar100.pt           --cuda yes --seed 0\"  # TODO: MODIFIED CHECK\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qs3rJGci3bH",
        "outputId": "d9747f80-3d8b-425b-975c-aec5e9607834",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 31
        }
      },
      "source": [
        "%cd /content/drive/My\\ Drive/DL"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/DL\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iusn_FmalZ9W",
        "outputId": "7a639c40-bfd1-4dd7-b407-ee734f07acd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 45
        }
      },
      "source": [
        "# build datasets\n",
        "%cd data/\n",
        "%cd raw/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/DL/data\n",
            "/content/drive/My Drive/DL/data/raw\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKvUCeczUxAs"
      },
      "source": [
        "# DOWNLOADING CIFAR and split in to MNIST TRAIN AND TEST DATASET\n",
        "# Executing Raw.py\n",
        "\n",
        "\n",
        "*   Downloading Data - cifar-100-python.tar.gz,  mnist.npz\n",
        "*   Output mnist_train.pt and mnist_test.pt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuXYI1EcU3UX"
      },
      "source": [
        "import numpy as np\n",
        "import subprocess\n",
        "import pickle\n",
        "import torch\n",
        "import os\n",
        "\n",
        "cifar_path = \"cifar-100-python.tar.gz\"\n",
        "mnist_path = \"mnist.npz\"\n",
        "\n",
        "# URL from: https://www.cs.toronto.edu/~kriz/cifar.html\n",
        "if not os.path.exists(cifar_path):\n",
        "    subprocess.call(\"wget https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\", shell=True)\n",
        "\n",
        "subprocess.call(\"tar xzfv cifar-100-python.tar.gz\", shell=True)\n",
        "\n",
        "# URL from: https://github.com/fchollet/keras/blob/master/keras/datasets/mnist.py\n",
        "if not os.path.exists(mnist_path):\n",
        "    subprocess.call(\"wget https://s3.amazonaws.com/img-datasets/mnist.npz\", shell=True)\n",
        "\n",
        "def unpickle(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict\n",
        "\n",
        "cifar100_train = unpickle('cifar-100-python/train')\n",
        "cifar100_test = unpickle('cifar-100-python/test')\n",
        "\n",
        "x_tr = torch.from_numpy(cifar100_train[b'data'])\n",
        "y_tr = torch.LongTensor(cifar100_train[b'fine_labels'])\n",
        "x_te = torch.from_numpy(cifar100_test[b'data'])\n",
        "y_te = torch.LongTensor(cifar100_test[b'fine_labels'])\n",
        "\n",
        "torch.save((x_tr, y_tr, x_te, y_te), 'cifar100.pt')\n",
        "\n",
        "f = np.load('mnist.npz')\n",
        "x_tr = torch.from_numpy(f['x_train'])\n",
        "y_tr = torch.from_numpy(f['y_train']).long()\n",
        "x_te = torch.from_numpy(f['x_test'])\n",
        "y_te = torch.from_numpy(f['y_test']).long()\n",
        "f.close()\n",
        "\n",
        "torch.save((x_tr, y_tr), 'mnist_train.pt')\n",
        "torch.save((x_te, y_te), 'mnist_test.pt')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMIGMEZRl81D"
      },
      "source": [
        "#import raw\n",
        "!python raw.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cxl8HNpWuez"
      },
      "source": [
        "# DOWNLOADING MNIST PERMUTATIONS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPX_ojSHmN5l",
        "outputId": "2bd07e81-b020-4c37-b844-f9ef34bbf871",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 31
        }
      },
      "source": [
        "%cd .."
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/DL/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nbz5Rg_ZW5vi"
      },
      "source": [
        "import argparse\n",
        "import os.path\n",
        "import torch\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "parser.add_argument('--i', default='raw/', help='input directory')\n",
        "parser.add_argument('--o', default='mnist_permutations.pt', help='output file')\n",
        "parser.add_argument('--n_tasks', default=3, type=int, help='number of tasks')\n",
        "parser.add_argument('--seed', default=0, type=int, help='random seed')\n",
        "args = parser.parse_args(args=[]) #TODO: MODIFIED CHECK\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "tasks_tr = []\n",
        "tasks_te = []\n",
        "\n",
        "x_tr, y_tr = torch.load(os.path.join(args.i, 'mnist_train.pt'))\n",
        "x_te, y_te = torch.load(os.path.join(args.i, 'mnist_test.pt'))\n",
        "x_tr = x_tr.float().view(x_tr.size(0), -1) / 255.0\n",
        "x_te = x_te.float().view(x_te.size(0), -1) / 255.0\n",
        "y_tr = y_tr.view(-1).long()\n",
        "y_te = y_te.view(-1).long()\n",
        "\n",
        "for t in range(args.n_tasks):\n",
        "    p = torch.randperm(x_tr.size(1)).long().view(-1)\n",
        "\n",
        "    tasks_tr.append(['random permutation', x_tr.index_select(1, p), y_tr])\n",
        "    tasks_te.append(['random permutation', x_te.index_select(1, p), y_te])\n",
        "\n",
        "torch.save([tasks_tr, tasks_te], args.o)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSssWVjrpBLi"
      },
      "source": [
        "#pwd\n",
        "$MY_PYTHON mnist_permutations.py \\\n",
        "\t--o mnist_permutations.pt \\\n",
        "\t--seed 0 \\\n",
        "\t--n_tasks 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8txbUlbOXbK6"
      },
      "source": [
        "# DOWNLOADING MNIST ROTATIONS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIMibGUeXmYm"
      },
      "source": [
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import argparse\n",
        "import os.path\n",
        "import random\n",
        "import torch\n",
        "\n",
        "\n",
        "def rotate_dataset(d, rotation):\n",
        "    result = torch.FloatTensor(d.size(0), 784)\n",
        "    tensor = transforms.ToTensor()\n",
        "\n",
        "    for i in range(d.size(0)):\n",
        "        img = Image.fromarray(d[i].numpy(), mode='L')\n",
        "        result[i] = tensor(img.rotate(rotation)).view(784)\n",
        "    return result\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "parser.add_argument('--i', default='raw/', help='input directory')\n",
        "parser.add_argument('--o', default='mnist_rotations.pt', help='output file')\n",
        "parser.add_argument('--n_tasks', default=10, type=int, help='number of tasks')\n",
        "parser.add_argument('--min_rot', default=0.,\n",
        "                    type=float, help='minimum rotation')\n",
        "parser.add_argument('--max_rot', default=90.,\n",
        "                    type=float, help='maximum rotation')\n",
        "parser.add_argument('--seed', default=0, type=int, help='random seed')\n",
        "\n",
        "args = parser.parse_args(args=[]) #TODO : MODIFIED CHECK\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "tasks_tr = []\n",
        "tasks_te = []\n",
        "\n",
        "x_tr, y_tr = torch.load(os.path.join(args.i, 'mnist_train.pt'))\n",
        "x_te, y_te = torch.load(os.path.join(args.i, 'mnist_test.pt'))\n",
        "\n",
        "for t in range(args.n_tasks):\n",
        "    min_rot = 1.0 * t / args.n_tasks * (args.max_rot - args.min_rot) + \\\n",
        "        args.min_rot\n",
        "    max_rot = 1.0 * (t + 1) / args.n_tasks * \\\n",
        "        (args.max_rot - args.min_rot) + args.min_rot\n",
        "    rot = random.random() * (max_rot - min_rot) + min_rot\n",
        "\n",
        "    tasks_tr.append([rot, rotate_dataset(x_tr, rot), y_tr])\n",
        "    tasks_te.append([rot, rotate_dataset(x_te, rot), y_te])\n",
        "\n",
        "torch.save([tasks_tr, tasks_te], args.o)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OReMYe7Zq501"
      },
      "source": [
        "!python mnist_rotations.py \\\n",
        "\t--o mnist_rotations.pt\\\n",
        "\t--seed 0 \\\n",
        "\t--min_rot 0 \\\n",
        "\t--max_rot 180 \\\n",
        "\t--n_tasks 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MvTRfJvbD7T"
      },
      "source": [
        "# PROCESSING CIFAR100"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4K5J8BO5bRyO",
        "outputId": "b1871be7-0fec-4207-bce6-d574e9b432d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "import argparse\n",
        "import os.path\n",
        "import torch\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "parser.add_argument('--i', default='raw/cifar100.pt', help='input directory')\n",
        "parser.add_argument('--o', default='cifar100.pt', help='output file')\n",
        "parser.add_argument('--n_tasks', default=10, type=int, help='number of tasks')\n",
        "parser.add_argument('--seed', default=0, type=int, help='random seed')\n",
        "args = parser.parse_args(args=[]) #TODO : MODIFIED CHECK\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "tasks_tr = []\n",
        "tasks_te = []\n",
        "\n",
        "x_tr, y_tr, x_te, y_te = torch.load(os.path.join(args.i))\n",
        "x_tr = x_tr.float().view(x_tr.size(0), -1) / 255.0\n",
        "x_te = x_te.float().view(x_te.size(0), -1) / 255.0\n",
        "\n",
        "cpt = int(100 / args.n_tasks)\n",
        "\n",
        "for t in range(args.n_tasks):\n",
        "    c1 = t * cpt\n",
        "    c2 = (t + 1) * cpt\n",
        "    i_tr = ((y_tr >= c1) & (y_tr < c2)).nonzero().view(-1)\n",
        "    i_te = ((y_te >= c1) & (y_te < c2)).nonzero().view(-1)\n",
        "    tasks_tr.append([(c1, c2), x_tr[i_tr].clone(), y_tr[i_tr].clone()])\n",
        "    tasks_te.append([(c1, c2), x_te[i_te].clone(), y_te[i_te].clone()])\n",
        "\n",
        "torch.save([tasks_tr, tasks_te], args.o)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: This overload of nonzero is deprecated:\n",
            "\tnonzero()\n",
            "Consider using one of the following signatures instead:\n",
            "\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcFbFJKtr6Bx",
        "outputId": "3bae0427-7a1f-457d-cda9-df890bc274fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python cifar100.py \\\n",
        "\t--o cifar100.pt \\\n",
        "\t--seed 0 \\\n",
        "\t--n_tasks 20"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cifar100.py:33: UserWarning: This overload of nonzero is deprecated:\n",
            "\tnonzero()\n",
            "Consider using one of the following signatures instead:\n",
            "\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  i_tr = ((y_tr >= c1) & (y_tr < c2)).nonzero().view(-1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xt6sMcFGsEpd",
        "outputId": "3a2d1b49-a422-427c-e763-acf0e31242ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%pwd"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/DL'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9vb3sdOs94P",
        "outputId": "5e002b54-9388-42f2-fe5b-9b5387cbd70d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 31
        }
      },
      "source": [
        "%cd metrics"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/DL/metrics\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7uexx0xtDE5"
      },
      "source": [
        "!python metrics.py"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cs8tTLY9tgS8",
        "outputId": "5c4de20a-0ed0-4fc7-9252-786e9efa7099",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%pwd"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/DL/metrics'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9As0_kP_uX38",
        "outputId": "5621020a-5a9b-412d-9fce-e2fa473d8712",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 31
        }
      },
      "source": [
        "%cd .."
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/DL\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fN4bXrv9cT3P"
      },
      "source": [
        "# MAIN : STOCHASTIC GRADIENT DESCENT - CONTINUAL LEARNING\n",
        "# MODELS\n",
        "\n",
        "\n",
        "1.   Single\n",
        "2.   Multimodal\n",
        "3. independent\n",
        "4.ewc - Elastic Weight Consolidation\n",
        "5.gem - Gradient Episodic Memory\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iF82WxiXcNT5",
        "outputId": "6b550359-97e6-4b93-9ea6-9b668a9ee64a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import importlib\n",
        "import datetime\n",
        "import argparse\n",
        "import random\n",
        "import uuid\n",
        "import time\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from metrics import confusion_matrix #TODO : MODIFIED\n",
        "\n",
        "# continuum iterator #########################################################\n",
        "\n",
        "\n",
        "def load_datasets(args):\n",
        "    d_tr, d_te = torch.load(args.data_path + '/' + args.data_file)\n",
        "    n_inputs = d_tr[0][1].size(1)\n",
        "    n_outputs = 0\n",
        "    for i in range(len(d_tr)):\n",
        "        n_outputs = max(n_outputs, d_tr[i][2].max().item())\n",
        "        n_outputs = max(n_outputs, d_te[i][2].max().item())\n",
        "    return d_tr, d_te, n_inputs, n_outputs + 1, len(d_tr)\n",
        "\n",
        "\n",
        "class Continuum:\n",
        "\n",
        "    def __init__(self, data, args):\n",
        "        self.data = data\n",
        "        self.batch_size = args.batch_size\n",
        "        n_tasks = len(data)\n",
        "        task_permutation = range(n_tasks)\n",
        "\n",
        "        if args.shuffle_tasks == 'yes':\n",
        "            task_permutation = torch.randperm(n_tasks).tolist()\n",
        "\n",
        "        sample_permutations = []\n",
        "\n",
        "        for t in range(n_tasks):\n",
        "            N = data[t][1].size(0)\n",
        "            if args.samples_per_task <= 0:\n",
        "                n = N\n",
        "            else:\n",
        "                n = min(args.samples_per_task, N)\n",
        "\n",
        "            p = torch.randperm(N)[0:n]\n",
        "            sample_permutations.append(p)\n",
        "\n",
        "        self.permutation = []\n",
        "\n",
        "        for t in range(n_tasks):\n",
        "            task_t = task_permutation[t]\n",
        "            for _ in range(args.n_epochs):\n",
        "                task_p = [[task_t, i] for i in sample_permutations[task_t]]\n",
        "                random.shuffle(task_p)\n",
        "                self.permutation += task_p\n",
        "\n",
        "        self.length = len(self.permutation)\n",
        "        self.current = 0\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def next(self):\n",
        "        return self.__next__()\n",
        "\n",
        "    def __next__(self):\n",
        "        if self.current >= self.length:\n",
        "            raise StopIteration\n",
        "        else:\n",
        "            ti = self.permutation[self.current][0]\n",
        "            j = []\n",
        "            i = 0\n",
        "            while (((self.current + i) < self.length) and\n",
        "                   (self.permutation[self.current + i][0] == ti) and\n",
        "                   (i < self.batch_size)):\n",
        "                j.append(self.permutation[self.current + i][1])\n",
        "                i += 1\n",
        "            self.current += i\n",
        "            j = torch.LongTensor(j)\n",
        "            return self.data[ti][1][j], ti, self.data[ti][2][j]\n",
        "\n",
        "# train handle ###############################################################\n",
        "\n",
        "\n",
        "def eval_tasks(model, tasks, args):\n",
        "    model.eval()\n",
        "    result = []\n",
        "    for i, task in enumerate(tasks):\n",
        "        t = i\n",
        "        x = task[1]\n",
        "        y = task[2]\n",
        "        rt = 0\n",
        "        \n",
        "        eval_bs = x.size(0)\n",
        "\n",
        "        for b_from in range(0, x.size(0), eval_bs):\n",
        "            b_to = min(b_from + eval_bs, x.size(0) - 1)\n",
        "            if b_from == b_to:\n",
        "                xb = x[b_from].view(1, -1)\n",
        "                yb = torch.LongTensor([y[b_to]]).view(1, -1)\n",
        "            else:\n",
        "                xb = x[b_from:b_to]\n",
        "                yb = y[b_from:b_to]\n",
        "            if args.cuda:\n",
        "                xb = xb.cuda()\n",
        "            _, pb = torch.max(model(xb, t).data.cpu(), 1, keepdim=False)\n",
        "            rt += (pb == yb).float().sum()\n",
        "\n",
        "        result.append(rt / x.size(0))\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def life_experience(model, continuum, x_te, args):\n",
        "    result_a = []\n",
        "    result_t = []\n",
        "\n",
        "    current_task = 0\n",
        "    time_start = time.time()\n",
        "\n",
        "    for (i, (x, t, y)) in enumerate(continuum):\n",
        "        if(((i % args.log_every) == 0) or (t != current_task)):\n",
        "            result_a.append(eval_tasks(model, x_te, args))\n",
        "            result_t.append(current_task)\n",
        "            current_task = t\n",
        "\n",
        "        v_x = x.view(x.size(0), -1)\n",
        "        v_y = y.long()\n",
        "\n",
        "        if args.cuda:\n",
        "            v_x = v_x.cuda()\n",
        "            v_y = v_y.cuda()\n",
        "\n",
        "        model.train()\n",
        "        model.observe(v_x, t, v_y)\n",
        "\n",
        "    result_a.append(eval_tasks(model, x_te, args))\n",
        "    result_t.append(current_task)\n",
        "\n",
        "    time_end = time.time()\n",
        "    time_spent = time_end - time_start\n",
        "\n",
        "    return torch.Tensor(result_t), torch.Tensor(result_a), time_spent\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description='Continuum learning')\n",
        "\n",
        "    # model parameters\n",
        "    parser.add_argument('--model', type=str, default='single',\n",
        "                        help='model to train')\n",
        "    parser.add_argument('--n_hiddens', type=int, default=100,\n",
        "                        help='number of hidden neurons at each layer')\n",
        "    parser.add_argument('--n_layers', type=int, default=2,\n",
        "                        help='number of hidden layers')\n",
        "\n",
        "    # memory parameters\n",
        "    parser.add_argument('--n_memories', type=int, default=0,\n",
        "                        help='number of memories per task')\n",
        "    parser.add_argument('--memory_strength', default=0, type=float,\n",
        "                        help='memory strength (meaning depends on memory)')\n",
        "    parser.add_argument('--finetune', default='no', type=str,\n",
        "                        help='whether to initialize nets in indep. nets')\n",
        "\n",
        "    # optimizer parameters\n",
        "    parser.add_argument('--n_epochs', type=int, default=1,\n",
        "                        help='Number of epochs per task')\n",
        "    parser.add_argument('--batch_size', type=int, default=10,\n",
        "                        help='batch size')\n",
        "    parser.add_argument('--lr', type=float, default=1e-3,\n",
        "                        help='SGD learning rate')\n",
        "\n",
        "    # experiment parameters\n",
        "    parser.add_argument('--cuda', type=str, default='no',\n",
        "                        help='Use GPU?')\n",
        "    parser.add_argument('--seed', type=int, default=0,\n",
        "                        help='random seed')\n",
        "    parser.add_argument('--log_every', type=int, default=100,\n",
        "                        help='frequency of logs, in minibatches')\n",
        "    parser.add_argument('--save_path', type=str, default='results/',\n",
        "                        help='save models at the end of training')\n",
        "\n",
        "    # data parameters\n",
        "    parser.add_argument('--data_path', default='data/',\n",
        "                        help='path where data is located')\n",
        "    parser.add_argument('--data_file', default='mnist_permutations.pt',\n",
        "                        help='data file')\n",
        "    parser.add_argument('--samples_per_task', type=int, default=-1,\n",
        "                        help='training samples per task (all if negative)')\n",
        "    parser.add_argument('--shuffle_tasks', type=str, default='no',\n",
        "                        help='present tasks in order')\n",
        "    args = parser.parse_args(args=[]) # TODO : MODIFIED CHECK\n",
        "\n",
        "    args.cuda = True if args.cuda == 'yes' else False\n",
        "    args.finetune = True if args.finetune == 'yes' else False\n",
        "\n",
        "    # multimodal model has one extra layer\n",
        "    if args.model == 'multimodal':\n",
        "        args.n_layers -= 1\n",
        "\n",
        "    # unique identifier\n",
        "    uid = uuid.uuid4().hex\n",
        "\n",
        "    # initialize seeds\n",
        "    torch.backends.cudnn.enabled = False\n",
        "    torch.manual_seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    random.seed(args.seed)\n",
        "    if args.cuda:\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "    # load data\n",
        "    x_tr, x_te, n_inputs, n_outputs, n_tasks = load_datasets(args)\n",
        "\n",
        "    # set up continuum\n",
        "    continuum = Continuum(x_tr, args)\n",
        "\n",
        "    # load model\n",
        "    Model = importlib.import_module('model.' + args.model)\n",
        "    model = Model.Net(n_inputs, n_outputs, n_tasks, args)\n",
        "    if args.cuda:\n",
        "        model.cuda()\n",
        "\n",
        "    # run model on continuum\n",
        "    result_t, result_a, spent_time = life_experience(\n",
        "        model, continuum, x_te, args)\n",
        "\n",
        "    # prepare saving path and file name\n",
        "    if not os.path.exists(args.save_path):\n",
        "        os.makedirs(args.save_path)\n",
        "\n",
        "    fname = args.model + '_' + args.data_file + '_'\n",
        "    fname += datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
        "    fname += '_' + uid\n",
        "    fname = os.path.join(args.save_path, fname)\n",
        "\n",
        "    # save confusion matrix and print one line of stats\n",
        "    stats = confusion_matrix(result_t, result_a, fname + '.txt')\n",
        "    one_liner = str(vars(args)) + ' # '\n",
        "    one_liner += ' '.join([\"%.3f\" % stat for stat in stats])\n",
        "    print(fname + ': ' + one_liner + ' # ' + str(spent_time))\n",
        "\n",
        "    # save all results in binary file\n",
        "    torch.save((result_t, result_a, model.state_dict(),\n",
        "                stats, one_liner, args), fname + '.pt')\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "results/single_mnist_permutations.pt_2020_10_17_13_23_18_a0059f10775e4a0a88ecf6f6e063916e: {'model': 'single', 'n_hiddens': 100, 'n_layers': 2, 'n_memories': 0, 'memory_strength': 0, 'finetune': False, 'n_epochs': 1, 'batch_size': 10, 'lr': 0.001, 'cuda': False, 'seed': 0, 'log_every': 100, 'save_path': 'results/', 'data_path': 'data/', 'data_file': 'mnist_permutations.pt', 'samples_per_task': -1, 'shuffle_tasks': 'no'} # 0.832 -0.049 -0.010 # 39.38511252403259\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPpuKJ8Ybown"
      },
      "source": [
        "# CONTINUAL LEARNING : SINGLE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4q9HeJTscrR",
        "outputId": "c9cf0040-696e-452d-9bc8-9d6c6e98ed49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!python main.py $MNIST_ROTA --model single --lr 0.003"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "results/single_mnist_rotations.pt_2020_10_17_13_24_39_8f3035f3bee44f1aa3c627e3c5d9dbc6: {'model': 'single', 'n_hiddens': 100, 'n_layers': 2, 'n_memories': 0, 'memory_strength': 0, 'finetune': False, 'n_epochs': 1, 'batch_size': 10, 'lr': 0.003, 'cuda': False, 'seed': 0, 'log_every': 100, 'save_path': 'results/', 'data_path': 'data/', 'data_file': 'mnist_rotations.pt', 'samples_per_task': 1000, 'shuffle_tasks': 'no'} # 0.574 0.086 0.276 # 5.51280403137207\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66DAXknXvFj1",
        "outputId": "17dd17a1-41d2-4862-be0d-d2cfaa8d31c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python main.py $MNIST_PERM --model single --lr 0.03"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "results/single_mnist_permutations.pt_2020_10_07_01_46_29_fedc5f4277e748f78d5c439342ba728b: {'model': 'single', 'n_hiddens': 100, 'n_layers': 2, 'n_memories': 0, 'memory_strength': 0, 'finetune': False, 'n_epochs': 1, 'batch_size': 10, 'lr': 0.03, 'cuda': False, 'seed': 0, 'log_every': 100, 'save_path': 'results/', 'data_path': 'data/', 'data_file': 'mnist_permutations.pt', 'samples_per_task': 1000, 'shuffle_tasks': 'no'} # 0.724 -0.031 -0.017 # 0.8295390605926514\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCBvWrhQvh3N",
        "outputId": "956bd694-01af-4bdc-e699-66cfef9ade8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python main.py $CIFAR_100i --model single --lr 1.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"main.py\", line 234, in <module>\n",
            "    model, continuum, x_te, args)\n",
            "  File \"main.py\", line 145, in life_experience\n",
            "    result_a.append(eval_tasks(model, x_te, args))\n",
            "  File \"main.py\", line 114, in eval_tasks\n",
            "    _, pb = torch.max(model(xb, t).data.cpu(), 1, keepdim=False)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 722, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/content/drive/My Drive/DL/model/single.py\", line 50, in forward\n",
            "    output = self.net(x)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 722, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/content/drive/My Drive/DL/model/common.py\", line 97, in forward\n",
            "    out = self.layer4(out)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 722, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\", line 117, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 722, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/content/drive/My Drive/DL/model/common.py\", line 64, in forward\n",
            "    out = self.bn2(self.conv2(out))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 722, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/batchnorm.py\", line 136, in forward\n",
            "    self.weight, self.bias, bn_training, exponential_average_factor, self.eps)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\", line 2016, in batch_norm\n",
            "    training, momentum, eps, torch.backends.cudnn.enabled\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DpOzMtYvXSB",
        "outputId": "5b81b08f-be28-4447-dbdc-73c63a1601bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 45
        }
      },
      "source": [
        "pip install quadprog"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: quadprog in /usr/local/lib/python3.6/dist-packages (0.1.7)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (from quadprog) (0.29.21)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOIFnGBOvem9",
        "outputId": "80aa96b8-42fb-457e-a30c-f9d4f4355a53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 31
        }
      },
      "source": [
        "#!python commmon.py\n",
        "%pwd\n",
        "%cd '/content/drive/My Drive/DL/model'"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/DL/model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFM-_aGhvjdN"
      },
      "source": [
        "!python common.py"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1LEEFMTv1Iq"
      },
      "source": [
        "# Copyright 2017-present, Facebook, Inc.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.functional import relu, avg_pool2d\n",
        "\n",
        "\n",
        "def Xavier(m):\n",
        "    if m.__class__.__name__ == 'Linear':\n",
        "        fan_in, fan_out = m.weight.data.size(1), m.weight.data.size(0)\n",
        "        std = 1.0 * math.sqrt(2.0 / (fan_in + fan_out))\n",
        "        a = math.sqrt(3.0) * std\n",
        "        m.weight.data.uniform_(-a, a)\n",
        "        m.bias.data.fill_(0.0)\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, sizes):\n",
        "        super(MLP, self).__init__()\n",
        "        layers = []\n",
        "\n",
        "        for i in range(0, len(sizes) - 1):\n",
        "            layers.append(nn.Linear(sizes[i], sizes[i + 1]))\n",
        "            if i < (len(sizes) - 2):\n",
        "                layers.append(nn.ReLU())\n",
        "\n",
        "        self.net = nn.Sequential(*layers)\n",
        "        self.net.apply(Xavier)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(in_planes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1,\n",
        "                          stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion * planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes, nf):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = nf\n",
        "\n",
        "        self.conv1 = conv3x3(3, nf * 1)\n",
        "        self.bn1 = nn.BatchNorm2d(nf * 1)\n",
        "        self.layer1 = self._make_layer(block, nf * 1, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, nf * 2, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, nf * 4, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, nf * 8, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(nf * 8 * block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        bsz = x.size(0)\n",
        "        out = relu(self.bn1(self.conv1(x.view(bsz, 3, 32, 32))))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18(nclasses, nf=20):\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2], nclasses, nf)\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yglQ5WcIvlcR",
        "outputId": "34498ae7-751b-4014-eb25-3e7dc93bed4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 31
        }
      },
      "source": [
        "%cd .."
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/DL\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHrOVaTuxwxs",
        "outputId": "50db4863-5e19-4d48-fe25-67baab6a471b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 31
        }
      },
      "source": [
        "%cd ../model"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/DL/model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaaT0At-yVX2",
        "outputId": "57fdea92-afa0-49fa-e129-83841c37c248",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 31
        }
      },
      "source": [
        "%cd .."
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/DL\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fdbNhZu181e"
      },
      "source": [
        "# Continual learning : EWC\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cXr45ctx6yn",
        "outputId": "2113d3bf-f0b0-4944-b3e7-339d2f50df43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!python main.py $MNIST_ROTA --model ewc --lr 0.01 --n_memories 1000 --memory_strength 1000"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "results/ewc_mnist_rotations.pt_2020_10_17_13_40_04_634158387df741a5987b9fa4376669f0: {'model': 'ewc', 'n_hiddens': 100, 'n_layers': 2, 'n_memories': 1000, 'memory_strength': 1000.0, 'finetune': False, 'n_epochs': 1, 'batch_size': 10, 'lr': 0.01, 'cuda': False, 'seed': 0, 'log_every': 100, 'save_path': 'results/', 'data_path': 'data/', 'data_file': 'mnist_rotations.pt', 'samples_per_task': 1000, 'shuffle_tasks': 'no'} # 0.624 -0.107 0.495 # 13.038573980331421\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMW0ijO3yf75",
        "outputId": "3f75eae2-163b-4a68-cb69-fc4b43abb882",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!python main.py $MNIST_PERM --model ewc --lr 0.1  --n_memories 10   --memory_strength 3"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "results/ewc_mnist_permutations.pt_2020_10_17_13_40_28_69460d616f9440199d92ef8a7a27e9f8: {'model': 'ewc', 'n_hiddens': 100, 'n_layers': 2, 'n_memories': 10, 'memory_strength': 3.0, 'finetune': False, 'n_epochs': 1, 'batch_size': 10, 'lr': 0.1, 'cuda': False, 'seed': 0, 'log_every': 100, 'save_path': 'results/', 'data_path': 'data/', 'data_file': 'mnist_permutations.pt', 'samples_per_task': 1000, 'shuffle_tasks': 'no'} # 0.785 -0.026 -0.021 # 1.2530765533447266\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAxvmk21yjpc"
      },
      "source": [
        "!python main.py $CIFAR_100i --model ewc --lr 1.0  --n_memories 10   --memory_strength 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMWX6UfY2ZTK",
        "outputId": "e1ac8db6-5832-4c14-8fd8-1318ae4414cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 31
        }
      },
      "source": [
        "%cd .."
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/DL\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBO8DPjr2Oni"
      },
      "source": [
        "# MULTI MODAL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLv3rW6w2Sax",
        "outputId": "1519fa1e-49ff-4042-c549-0ebe032b4183",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        }
      },
      "source": [
        "# model \"multimodal\"\n",
        "!python main.py $MNIST_ROTA  --model multimodal --lr 0.1\n",
        "!python main.py $MNIST_PERM  --model multimodal --lr 0.1"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "results/multimodal_mnist_rotations.pt_2020_10_17_13_57_31_0f33994c2eae4d34b7b4da279269c6d3: {'model': 'multimodal', 'n_hiddens': 100, 'n_layers': 1, 'n_memories': 0, 'memory_strength': 0, 'finetune': False, 'n_epochs': 1, 'batch_size': 10, 'lr': 0.1, 'cuda': False, 'seed': 0, 'log_every': 100, 'save_path': 'results/', 'data_path': 'data/', 'data_file': 'mnist_rotations.pt', 'samples_per_task': 1000, 'shuffle_tasks': 'no'} # 0.787 0.000 0.023 # 6.4455726146698\n",
            "results/multimodal_mnist_permutations.pt_2020_10_17_13_57_35_068542a896874896972e5ff2d49bd9c7: {'model': 'multimodal', 'n_hiddens': 100, 'n_layers': 1, 'n_memories': 0, 'memory_strength': 0, 'finetune': False, 'n_epochs': 1, 'batch_size': 10, 'lr': 0.1, 'cuda': False, 'seed': 0, 'log_every': 100, 'save_path': 'results/', 'data_path': 'data/', 'data_file': 'mnist_permutations.pt', 'samples_per_task': 1000, 'shuffle_tasks': 'no'} # 0.789 0.031 0.015 # 0.8775877952575684\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFaxKnc52g-G"
      },
      "source": [
        "# INDEPENDENT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WOuL2WS2j_2",
        "outputId": "5c131290-d75a-40d7-91f2-1801f4277340",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        }
      },
      "source": [
        "# model \"independent\"\n",
        "!python main.py $MNIST_ROTA --model independent --lr 0.1  --finetune yes \n",
        "!python main.py $MNIST_PERM --model independent --lr 0.03 --finetune yes \n"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "results/independent_mnist_rotations.pt_2020_10_17_13_58_31_0ffd3057fb0341f99b5d75eac0643371: {'model': 'independent', 'n_hiddens': 100, 'n_layers': 2, 'n_memories': 0, 'memory_strength': 0, 'finetune': True, 'n_epochs': 1, 'batch_size': 10, 'lr': 0.1, 'cuda': False, 'seed': 0, 'log_every': 100, 'save_path': 'results/', 'data_path': 'data/', 'data_file': 'mnist_rotations.pt', 'samples_per_task': 1000, 'shuffle_tasks': 'no'} # 0.739 0.000 0.000 # 2.529115915298462\n",
            "results/independent_mnist_permutations.pt_2020_10_17_13_58_34_93eee9e8bd1e44499d273d39ab71cac1: {'model': 'independent', 'n_hiddens': 100, 'n_layers': 2, 'n_memories': 0, 'memory_strength': 0, 'finetune': True, 'n_epochs': 1, 'batch_size': 10, 'lr': 0.03, 'cuda': False, 'seed': 0, 'log_every': 100, 'save_path': 'results/', 'data_path': 'data/', 'data_file': 'mnist_permutations.pt', 'samples_per_task': 1000, 'shuffle_tasks': 'no'} # 0.728 0.000 0.000 # 0.5108816623687744\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaXxVH2T2kr1"
      },
      "source": [
        "!python main.py $CIFAR_100i --model independent --lr 0.3  --finetune yes "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fUetW31pzmT"
      },
      "source": [
        "# Continual Learning : GEM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0rHSoVopwY3",
        "outputId": "2703573c-d3e9-4517-ccd6-33f49a771aa6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# model \"GEM\"\n",
        "!python main.py $MNIST_ROTA --model gem --lr 0.1 --n_memories 256 --memory_strength 0.5\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "results/gem_mnist_rotations.pt_2020_10_17_13_31_27_ac1f9256f66540009881eb50bfc12dfc: {'model': 'gem', 'n_hiddens': 100, 'n_layers': 2, 'n_memories': 256, 'memory_strength': 0.5, 'finetune': False, 'n_epochs': 1, 'batch_size': 10, 'lr': 0.1, 'cuda': False, 'seed': 0, 'log_every': 100, 'save_path': 'results/', 'data_path': 'data/', 'data_file': 'mnist_rotations.pt', 'samples_per_task': 1000, 'shuffle_tasks': 'no'} # 0.893 0.044 0.618 # 35.57505917549133\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZZYLy2vpxDt",
        "outputId": "c7bd6b40-8947-4d6a-f1c8-57e729551055",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!python main.py $MNIST_PERM --model gem --lr 0.1 --n_memories 256 --memory_strength 0.5"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "results/gem_mnist_permutations.pt_2020_10_17_13_31_49_0f0936a16c594a33b34b941ef91b21c5: {'model': 'gem', 'n_hiddens': 100, 'n_layers': 2, 'n_memories': 256, 'memory_strength': 0.5, 'finetune': False, 'n_epochs': 1, 'batch_size': 10, 'lr': 0.1, 'cuda': False, 'seed': 0, 'log_every': 100, 'save_path': 'results/', 'data_path': 'data/', 'data_file': 'mnist_permutations.pt', 'samples_per_task': 1000, 'shuffle_tasks': 'no'} # 0.839 0.027 -0.020 # 2.697176456451416\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKanbYAppxP5"
      },
      "source": [
        "!python main.py $CIFAR_100i --model gem --lr 0.1 --n_memories 256 --memory_strength 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPhqBufRgGFF"
      },
      "source": [
        "# RESULTS : EVALUATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sR_Y7IImugT7",
        "outputId": "cd79bd8c-d260-453e-f416-46d964de31ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd results/"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/DL/results\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97ZThoH0gVgO",
        "outputId": "2e91e15e-e879-4183-8568-d4537014fbef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import matplotlib as mpl\n",
        "mpl.use('Agg')\n",
        "\n",
        "# if 'roman' in mpl.font_manager.weight_dict.keys():\n",
        "#     del mpl.font_manager.weight_dict['roman']\n",
        "# mpl.font_manager._rebuild()\n",
        "\n",
        "mpl.rcParams[\"font.family\"] = \"Times New Roman\"\n",
        "mpl.rcParams[\"font.family\"] = \"DejaVu Serif\"\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "models = ['single', 'independent', 'multimodal', 'icarl', 'ewc', 'gem']\n",
        "datasets = ['mnist_permutations', 'mnist_rotations', 'cifar100']\n",
        "\n",
        "names_datasets = {'mnist_permutations': 'MNIST permutations',\n",
        "                  'mnist_rotations': 'MNIST rotations',\n",
        "                  'cifar100': 'CIFAR-100'}\n",
        "\n",
        "names_models = {'single': 'single',\n",
        "                'independent': 'independent',\n",
        "                'multimodal': 'multimodal',\n",
        "                'icarl': 'iCARL',\n",
        "                'ewc': 'EWC',\n",
        "                'gem': 'GEM'}\n",
        "\n",
        "colors = {'single': 'C0',\n",
        "          'independent': 'C1',\n",
        "          'multimodal': 'C2',\n",
        "          'icarl': 'C2',\n",
        "          'ewc': 'C3',\n",
        "          'gem': 'C4'}\n",
        "\n",
        "barplot = {}\n",
        "\n",
        "for dataset in datasets:\n",
        "    barplot[dataset] = {}\n",
        "    for model in models:\n",
        "        barplot[dataset][model] = {}\n",
        "        matches = glob(model + '*' + dataset + '*.pt')\n",
        "        if len(matches):\n",
        "            data = torch.load(matches[0], map_location=lambda storage, loc: storage)\n",
        "            acc, bwt, fwt = data[3][:]\n",
        "            barplot[dataset][model]['acc'] = acc\n",
        "            barplot[dataset][model]['bwt'] = bwt\n",
        "            barplot[dataset][model]['fwt'] = fwt\n",
        "\n",
        "for dataset in datasets:\n",
        "    x_lab = []\n",
        "    y_acc = []\n",
        "    y_bwt = []\n",
        "    y_fwt = []\n",
        "\n",
        "    for i, model in enumerate(models):\n",
        "        if barplot[dataset][model] != {}:\n",
        "            x_lab.append(model)\n",
        "            y_acc.append(barplot[dataset][model]['acc'])\n",
        "            y_bwt.append(barplot[dataset][model]['bwt'])\n",
        "            y_fwt.append(barplot[dataset][model]['fwt'])\n",
        "\n",
        "    x_ind = np.arange(len(y_acc))\n",
        "\n",
        "    plt.figure(figsize=(7, 3))\n",
        "    all_colors = []\n",
        "    for xi, yi, li in zip(x_ind, y_acc, x_lab):\n",
        "        plt.bar(xi, yi, label=names_models[li], color=colors[li])\n",
        "        all_colors.append(colors[li])\n",
        "    plt.bar(x_ind + (len(y_acc) + 1) * 1, y_bwt, color=all_colors)\n",
        "    plt.bar(x_ind + (len(y_acc) + 1) * 2, y_fwt, color=all_colors)\n",
        "    plt.xticks([2, 8, 14], ['ACC', 'BWT', 'FWT'], fontsize=16)\n",
        "    plt.yticks(fontsize=16)\n",
        "    plt.xlim(-1, len(y_acc) * 3 + 2)\n",
        "    plt.ylabel('classification accuracy', fontsize=16)\n",
        "    plt.title(names_datasets[dataset], fontsize=16)\n",
        "    plt.legend(fontsize=12)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('CS677_DL_Shiva_Kowsick_barplot_%s.pdf' % dataset, bbox_inches='tight')\n",
        "    # plt.show()\n",
        "\n",
        "evoplot = {}\n",
        "\n",
        "for dataset in datasets:\n",
        "    evoplot[dataset] = {}\n",
        "    for model in models:\n",
        "        matches = glob(model + '*' + dataset + '*.pt')\n",
        "        if len(matches):\n",
        "            data = torch.load(matches[0], map_location=lambda storage, loc: storage)\n",
        "            evoplot[dataset][model] = data[1][:, 0].numpy()\n",
        "\n",
        "for dataset in datasets:\n",
        "\n",
        "    plt.figure(figsize=(7, 3))\n",
        "    for model in models:\n",
        "        if model in evoplot[dataset]:\n",
        "            x = np.arange(len(evoplot[dataset][model]))\n",
        "            x = (x - x.min()) / (x.max() - x.min()) * 20\n",
        "            plt.plot(x, evoplot[dataset][model], color=colors[model], lw=3)\n",
        "            plt.xticks(range(0, 21, 2))\n",
        "\n",
        "    plt.xticks(fontsize=16)\n",
        "    plt.yticks(fontsize=16)\n",
        "    #plt.xlabel('task number', fontsize=16)\n",
        "    plt.title(names_datasets[dataset], fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('CS677_DL_Shiva_Kowsick_evoplot_%s.pdf' % dataset, bbox_inches='tight')\n",
        "    # plt.show()\n"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No handles with labels found to put in legend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_uZCR00uqiL",
        "outputId": "ffd27576-fd3d-4089-e7c1-2d52bbbb499c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python plot_results.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No handles with labels found to put in legend.\n",
            "No handles with labels found to put in legend.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}